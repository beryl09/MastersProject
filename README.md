# Abstract

Breast cancer remains one of the most prevalent and life-threatening diseases worldwide, necessitating accurate and timely diagnosis to improve patient survival rates. Traditional diagnostic methods, such as mammography, histopathology, and genomic analysis, often operate in isolation, limiting their effectiveness in capturing the full complexity of the disease. Recent advancements in artificial intelligence (AI) and machine learning (ML) have enabled the development of multi-modal approaches that integrate diverse data sources, including imaging, genomic markers, and clinical records, to enhance diagnostic precision.

This study explores the application of deep learning-based multi-modal fusion techniques for breast cancer classification. By leveraging convolutional neural networks (CNNs), Multi-layer Perceptron, and graph neural networks (GNNs), the research aims to improve the robustness and accuracy of tumor classification across different modalities. Additionally, explainable AI (XAI) techniques such as Grad-CAM, SHAP, and LIME are incorporated to enhance model transparency and trustworthiness in clinical decision-making.

The project investigates various fusion strategies, including early, late, and hybrid fusion, to determine the most effective integration method for multi-modal breast cancer diagnosis. The findings are expected to contribute to the development of AI-driven decision support systems that aid radiologists and oncologists in diagnosing and managing breast cancer more effectively. Ultimately, this research seeks to improve diagnostic accuracy, reduce false positives, and support personalized treatment strategies, advancing the role of AI in medical imaging and cancer detection.

# Introduction

Breast cancer remains one of the most prevalent and life-threatening diseases worldwide especially in women, necessitating early and accurate detection to improve patient survival rates. Traditional diagnostic approaches rely on imaging modalities such as digital mammography (DM), ultrasound, magnetic resonance imaging (MRI), and histopathology to identify malignant tumors. While these techniques are effective, they exhibit significant limitations. Mammography, the primary breast cancer screening tool, has reduced sensitivity in women with dense breast tissue, leading to a higher risk of false negatives. Additionally, histopathological examination of biopsy samples is heavily dependent on human expertise, making it susceptible to inter-observer variability and diagnostic inconsistencies​. These challenges highlight the need for more comprehensive and automated diagnostic solutions that leverage multiple data sources to enhance accuracy and reliability.

Advancements in artificial intelligence (AI) and machine learning (ML) have introduced promising solutions to breast cancer detection and classification. AI-driven models, particularly those based on deep learning architectures such as convolutional neural networks (CNNs) and transformer-based models, have demonstrated remarkable success in analyzing medical images, segmenting tumor regions, and classifying cancer subtypes​
. However, many of these AI models rely on single-modality datasets, which limits their ability to capture the full complexity of breast cancer. The integration of multi-modal data, combining imaging, genomic information, histopathology, and clinical records, offers a more holistic approach to cancer diagnosis. By leveraging deep learning architectures such as CNNs, graph neural networks (GNNs), and attention-based models, multi-modal AI systems can extract complementary features from various data sources, leading to improved diagnostic accuracy and robustness​.

Traditional unimodal approaches face several limitations. Digital mammography alone struggles with generalization across different patient populations, particularly when imaging conditions vary between medical centers. Histopathology-based diagnosis, while critical for confirming malignancy, requires time-intensive manual evaluation by pathologists. Furthermore, genomic data, which plays a key role in identifying tumor subtypes and treatment responses, is often underutilized in imaging-based AI models​. Additionally, many datasets used in AI training are imbalanced, containing more benign cases than malignant ones, leading to bias and reducing the model's effectiveness in detecting aggressive cancers​. These shortcomings emphasize the importance of integrating diverse modalities to improve model performance and clinical applicability.

Multi-modal AI systems address these challenges by combining different data types into a unified framework. The Duke Breast Cancer MRI Dataset exemplifies this approach by providing a rich collection of pre-operative breast MRI scans, genomic markers, and clinical data. Studies have shown that combining MRI with genomic and histopathology data enhances the predictive power of AI models, enabling them to detect molecular subtypes of breast cancer, classify HER2-positive, ER/PR-positive, and triple-negative cases, and predict patient outcomes and treatment responses​. This multi-modal approach improves the detection of aggressive tumors, reduces false positives, and provides more personalized treatment recommendations based on individual patient profiles.

Despite the success of AI in breast cancer detection, a major concern remains: the black-box nature of deep learning models. Clinicians require interpretability and transparency to trust AI-based decision-making. Explainable AI (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), SHapley Additive exPlanations (SHAP), and Local Interpretable Model-agnostic Explanations (LIME), help address this challenge by providing visual and analytical insights into the AI model’s decision-making process​. These methods enhance the interpretability of AI predictions, allowing clinicians to validate tumor detection, segmentation, and classification outputs and integrate them into clinical workflows more effectively.

This research aims to develop and evaluate a multi-modal AI framework for breast cancer classification, integrating MRI (from the Duke Breast Cancer MRI dataset), histopathology slides, genomic data, and clinical information. The study explores different fusion strategies—including early, intermediate, and late fusion—to determine the most effective integration approach for improving diagnostic accuracy. Furthermore, state-of-the-art deep learning architectures, including CNNs, graph neural networks (GNNs), and attention-based models, will be employed to classify tumors and predict clinical outcomes. Additionally, this research incorporates explainability techniques to improve model transparency and enhance clinician trust in AI-assisted decision-making.

By leveraging multi-modal AI techniques, this research aims to bridge the gap between AI advancements and real-world clinical applications, enhancing the accuracy, interpretability, and clinical utility of breast cancer diagnosis. The findings will contribute to the growing field of AI-driven precision medicine, demonstrating that the integration of imaging, genomic, and histopathology data can lead to improved patient outcomes, personalized treatment strategies, and reduced diagnostic errors. Ultimately, this study seeks to advance AI-assisted diagnostics and decision-support systems for breast cancer detection, setting the stage for future breakthroughs in multi-modal deep learning in oncology.

